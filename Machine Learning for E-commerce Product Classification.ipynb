{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5b0b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Etsy Product Classification Project\n",
      "Using Random Forest, SGD Classifier, and Naive Bayes models\n",
      "Loading 362 files from E:/sunday/data_2025/2025/train...\n",
      "Loaded 5/362 files\n",
      "Loaded 10/362 files\n",
      "Loaded 15/362 files\n",
      "Loaded 20/362 files\n",
      "Loaded 25/362 files\n",
      "Loaded 30/362 files\n",
      "Loaded 35/362 files\n",
      "Loaded 40/362 files\n",
      "Loaded 45/362 files\n",
      "Loaded 50/362 files\n",
      "Loaded 55/362 files\n",
      "Loaded 60/362 files\n",
      "Loaded 65/362 files\n",
      "Loaded 70/362 files\n",
      "Loaded 75/362 files\n",
      "Loaded 80/362 files\n",
      "Loaded 85/362 files\n",
      "Loaded 90/362 files\n",
      "Loaded 95/362 files\n",
      "Loaded 100/362 files\n",
      "Loaded 105/362 files\n",
      "Loaded 110/362 files\n",
      "Loaded 115/362 files\n",
      "Loaded 120/362 files\n",
      "Loaded 125/362 files\n",
      "Loaded 130/362 files\n",
      "Loaded 135/362 files\n",
      "Loaded 140/362 files\n",
      "Loaded 145/362 files\n",
      "Loaded 150/362 files\n",
      "Loaded 155/362 files\n",
      "Loaded 160/362 files\n",
      "Loaded 165/362 files\n",
      "Loaded 170/362 files\n",
      "Loaded 175/362 files\n",
      "Loaded 180/362 files\n",
      "Loaded 185/362 files\n",
      "Loaded 190/362 files\n",
      "Loaded 195/362 files\n",
      "Loaded 200/362 files\n",
      "Loaded 205/362 files\n",
      "Loaded 210/362 files\n",
      "Loaded 215/362 files\n",
      "Loaded 220/362 files\n",
      "Loaded 225/362 files\n",
      "Loaded 230/362 files\n",
      "Loaded 235/362 files\n",
      "Loaded 240/362 files\n",
      "Loaded 245/362 files\n",
      "Loaded 250/362 files\n",
      "Loaded 255/362 files\n",
      "Loaded 260/362 files\n",
      "Loaded 265/362 files\n",
      "Loaded 270/362 files\n",
      "Loaded 275/362 files\n",
      "Loaded 280/362 files\n",
      "Loaded 285/362 files\n",
      "Loaded 290/362 files\n",
      "Loaded 295/362 files\n",
      "Loaded 300/362 files\n",
      "Loaded 305/362 files\n",
      "Loaded 310/362 files\n",
      "Loaded 315/362 files\n",
      "Loaded 320/362 files\n",
      "Loaded 325/362 files\n",
      "Loaded 330/362 files\n",
      "Loaded 335/362 files\n",
      "Loaded 340/362 files\n",
      "Loaded 345/362 files\n",
      "Loaded 350/362 files\n",
      "Loaded 355/362 files\n",
      "Loaded 360/362 files\n",
      "Loading 362 files from E:/sunday/data_2025/2025/test...\n",
      "Loaded 5/362 files\n",
      "Loaded 10/362 files\n",
      "Loaded 15/362 files\n",
      "Loaded 20/362 files\n",
      "Loaded 25/362 files\n",
      "Loaded 30/362 files\n",
      "Loaded 35/362 files\n",
      "Loaded 40/362 files\n",
      "Loaded 45/362 files\n",
      "Loaded 50/362 files\n",
      "Loaded 55/362 files\n",
      "Loaded 60/362 files\n",
      "Loaded 65/362 files\n",
      "Loaded 70/362 files\n",
      "Loaded 75/362 files\n",
      "Loaded 80/362 files\n",
      "Loaded 85/362 files\n",
      "Loaded 90/362 files\n",
      "Loaded 95/362 files\n",
      "Loaded 100/362 files\n",
      "Loaded 105/362 files\n",
      "Loaded 110/362 files\n",
      "Loaded 115/362 files\n",
      "Loaded 120/362 files\n",
      "Loaded 125/362 files\n",
      "Loaded 130/362 files\n",
      "Loaded 135/362 files\n",
      "Loaded 140/362 files\n",
      "Loaded 145/362 files\n",
      "Loaded 150/362 files\n",
      "Loaded 155/362 files\n",
      "Loaded 160/362 files\n",
      "Loaded 165/362 files\n",
      "Loaded 170/362 files\n",
      "Loaded 175/362 files\n",
      "Loaded 180/362 files\n",
      "Loaded 185/362 files\n",
      "Loaded 190/362 files\n",
      "Loaded 195/362 files\n",
      "Loaded 200/362 files\n",
      "Loaded 205/362 files\n",
      "Loaded 210/362 files\n",
      "Loaded 215/362 files\n",
      "Loaded 220/362 files\n",
      "Loaded 225/362 files\n",
      "Loaded 230/362 files\n",
      "Loaded 235/362 files\n",
      "Loaded 240/362 files\n",
      "Loaded 245/362 files\n",
      "Loaded 250/362 files\n",
      "Loaded 255/362 files\n",
      "Loaded 260/362 files\n",
      "Loaded 265/362 files\n",
      "Loaded 270/362 files\n",
      "Loaded 275/362 files\n",
      "Loaded 280/362 files\n",
      "Loaded 285/362 files\n",
      "Loaded 290/362 files\n",
      "Loaded 295/362 files\n",
      "Loaded 300/362 files\n",
      "Loaded 305/362 files\n",
      "Loaded 310/362 files\n",
      "Loaded 315/362 files\n",
      "Loaded 320/362 files\n",
      "Loaded 325/362 files\n",
      "Loaded 330/362 files\n",
      "Loaded 335/362 files\n",
      "Loaded 340/362 files\n",
      "Loaded 345/362 files\n",
      "Loaded 350/362 files\n",
      "Loaded 355/362 files\n",
      "Loaded 360/362 files\n",
      "Train data shape: (229624, 23)\n",
      "Test data shape: (25514, 15)\n",
      "Training data shape: (229624, 23)\n",
      "Test data shape: (25514, 15)\n",
      "\n",
      "Missing values in train data:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "=== TEXT PREPROCESSING ===\n",
      "Applying text preprocessing...\n",
      "Text preprocessing completed\n",
      "\n",
      "Top category distribution:\n",
      "Number of unique top categories: 15\n",
      "Top 5 most common categories: \n",
      "top_category_id\n",
      "8     54600\n",
      "6     33393\n",
      "5     30143\n",
      "13    13835\n",
      "0     12416\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bottom category distribution:\n",
      "Number of unique bottom categories: 2609\n",
      "Top 5 most common categories: \n",
      "bottom_category_id\n",
      "2070     98\n",
      "46       98\n",
      "1046     98\n",
      "12186    98\n",
      "2371     98\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE ENGINEERING ===\n",
      "Encoding target variables...\n",
      "Number of unique top categories: 15\n",
      "Number of unique bottom categories: 2609\n",
      "Creating text features using hashing vectorizer...\n",
      "Hashed features shape: (229624, 2048)\n",
      "\n",
      "=== MODEL TRAINING - TOP CATEGORY ===\n",
      "Training set shape: (206661, 2048)\n",
      "Validation set shape: (22963, 2048)\n",
      "\n",
      "Performing dimensionality reduction for visualization...\n",
      "\n",
      "Training models for Top Category Classification...\n",
      "\n",
      "Training Random Forest for Top Category...\n",
      "Predicting with Random Forest...\n",
      "Random Forest F1 Score: 0.3245\n",
      "Training time: 48.81 seconds\n",
      "\n",
      "Training SGD Classifier for Top Category...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:738: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with SGD Classifier...\n",
      "SGD Classifier F1 Score: 0.3749\n",
      "Training time: 8.52 seconds\n",
      "\n",
      "Training Multinomial Naive Bayes for Top Category...\n",
      "Predicting with Multinomial Naive Bayes...\n",
      "Multinomial Naive Bayes F1 Score: 0.5652\n",
      "Training time: 0.85 seconds\n",
      "\n",
      "Training models for Bottom Category Classification...\n",
      "Using 200 common bottom categories\n",
      "Filtered training set shape: (17555, 2048)\n",
      "Filtered validation set shape: (1178, 2048)\n",
      "\n",
      "Training Random Forest for Bottom Category...\n",
      "Predicting with Random Forest...\n",
      "Random Forest F1 Score: 0.3135\n",
      "Training time: 3.92 seconds\n",
      "\n",
      "Training SGD Classifier for Bottom Category...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:738: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with SGD Classifier...\n",
      "SGD Classifier F1 Score: 0.6573\n",
      "Training time: 5.59 seconds\n",
      "\n",
      "Training Multinomial Naive Bayes for Bottom Category...\n",
      "Predicting with Multinomial Naive Bayes...\n",
      "Multinomial Naive Bayes F1 Score: 0.8202\n",
      "Training time: 0.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model for top category: Multinomial Naive Bayes (F1: 0.5652)\n",
      "Best model for bottom category: Multinomial Naive Bayes (F1: 0.8202)\n",
      "\n",
      "Making predictions on test data...\n",
      "\n",
      "Best model for top category: Multinomial Naive Bayes (F1: 0.5652)\n",
      "Best model for bottom category: Multinomial Naive Bayes (F1: 0.8202)\n",
      "Predictions saved to predictions.csv\n",
      "Visualizations saved to the 'visualizations' directory\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "Top Category Classification:\n",
      "  Random Forest: F1 Score = 0.3245, Training Time = 48.81s\n",
      "  SGD Classifier: F1 Score = 0.3749, Training Time = 8.52s\n",
      "  Multinomial Naive Bayes: F1 Score = 0.5652, Training Time = 0.85s\n",
      "\n",
      "Bottom Category Classification (filtered classes):\n",
      "  Random Forest: F1 Score = 0.3135, Training Time = 3.92s\n",
      "  SGD Classifier: F1 Score = 0.6573, Training Time = 5.59s\n",
      "  Multinomial Naive Bayes: F1 Score = 0.8202, Training Time = 0.81s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path settings - using forward slashes for consistency\n",
    "TRAIN_PATH = \"E:/sunday/data_2025/2025/train\"\n",
    "TEST_PATH = \"E:/sunday/data_2025/2025/test\"\n",
    "\n",
    "print(\"Starting Etsy Product Classification Project\")\n",
    "print(\"Using Random Forest, SGD Classifier, and Naive Bayes models\")\n",
    "\n",
    "##############################################\n",
    "# 1. DATA LOADING WITH MEMORY OPTIMIZATION\n",
    "##############################################\n",
    "def load_parquet_files(directory, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load parquet files with optional sampling for faster development\n",
    "    \"\"\"\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    if sample_size:\n",
    "        # For development: randomly select a subset of files\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        selected_files = random.sample(all_files, min(sample_size, len(all_files)))\n",
    "    else:\n",
    "        selected_files = all_files\n",
    "    \n",
    "    print(f\"Loading {len(selected_files)} files from {directory}...\")\n",
    "    \n",
    "    # Load files one by one to avoid memory issues\n",
    "    dataframes = []\n",
    "    for i, file in enumerate(selected_files):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            dataframes.append(df)\n",
    "            # Print progress every 5 files\n",
    "            if (i+1) % 5 == 0:\n",
    "                print(f\"Loaded {i+1}/{len(selected_files)} files\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load datasets with option to use a smaller sample for development\n",
    "use_sample = False  # Set to True for faster development with a sample\n",
    "sample_size = 5 if use_sample else None\n",
    "\n",
    "try:\n",
    "    train_df = load_parquet_files(TRAIN_PATH, sample_size)\n",
    "    test_df = load_parquet_files(TEST_PATH, sample_size)\n",
    "    print(f\"Train data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"\\nCreating synthetic data for demonstration purposes...\")\n",
    "    \n",
    "    # Create synthetic data for demonstration if files can't be loaded\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create dummy train data\n",
    "    n_samples = 5000\n",
    "    train_df = pd.DataFrame({\n",
    "        'product_id': [f\"prod_{i}\" for i in range(n_samples)],\n",
    "        'title': [f\"Product title {i}\" for i in range(n_samples)],\n",
    "        'description': [f\"Description for product {i} with details\" for i in range(n_samples)],\n",
    "        'tags': [f\"tag1, tag2, tag{i%10}\" for i in range(n_samples)],\n",
    "        'type': np.random.choice(['physical', 'download'], n_samples),\n",
    "        'room': np.random.choice(['bedroom', 'living room', 'kitchen', 'bathroom'], n_samples),\n",
    "        'top_category_id': np.random.choice([0, 5, 6, 8, 13], n_samples, p=[0.1, 0.25, 0.25, 0.3, 0.1]),\n",
    "        'top_category_text': np.random.choice(['category_0', 'category_5', 'category_6', 'category_8', 'category_13'], n_samples),\n",
    "        'bottom_category_id': np.random.randint(1, 100, n_samples),\n",
    "        'bottom_category_text': [f\"subcategory_{i%100}\" for i in range(n_samples)],\n",
    "        'primary_color_id': np.random.randint(1, 10, n_samples),\n",
    "        'primary_color_text': np.random.choice(['red', 'blue', 'green', 'yellow', 'black'], n_samples),\n",
    "        'secondary_color_id': np.random.randint(1, 10, n_samples),\n",
    "        'secondary_color_text': np.random.choice(['red', 'blue', 'green', 'yellow', 'black'], n_samples)\n",
    "    })\n",
    "    \n",
    "    # Create dummy test data\n",
    "    n_test = 1000\n",
    "    test_df = pd.DataFrame({\n",
    "        'product_id': [f\"test_prod_{i}\" for i in range(n_test)],\n",
    "        'title': [f\"Test Product title {i}\" for i in range(n_test)],\n",
    "        'description': [f\"Test Description for product {i} with details\" for i in range(n_test)],\n",
    "        'tags': [f\"tag1, tag2, tag{i%10}\" for i in range(n_test)],\n",
    "        'type': np.random.choice(['physical', 'download'], n_test),\n",
    "        'room': np.random.choice(['bedroom', 'living room', 'kitchen', 'bathroom'], n_test)\n",
    "    })\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "# Check missing values\n",
    "missing_values = train_df.isnull().sum()\n",
    "print(\"\\nMissing values in train data:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(df):\n",
    "    # Combine text fields efficiently\n",
    "    df['combined_text'] = df['title'].fillna('') + ' ' + df['description'].fillna('') + ' ' + df['tags'].fillna('')\n",
    "    \n",
    "    # Fill NaN values in categorical columns\n",
    "    text_cols = ['title', 'description', 'tags']\n",
    "    cat_cols = ['type', 'room', 'craft_type', 'recipient', 'material', 'occasion', \n",
    "                'holiday', 'art_subject', 'style', 'shape', 'pattern',\n",
    "                'primary_color_text', 'secondary_color_text']\n",
    "    \n",
    "    for col in text_cols + cat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess text\n",
    "print(\"\\n=== TEXT PREPROCESSING ===\")\n",
    "print(\"Applying text preprocessing...\")\n",
    "train_df = preprocess_text(train_df)\n",
    "test_df = preprocess_text(test_df)\n",
    "print(\"Text preprocessing completed\")\n",
    "\n",
    "# Explore target distributions\n",
    "print(\"\\nTop category distribution:\")\n",
    "top_cat_dist = train_df['top_category_id'].value_counts()\n",
    "print(f\"Number of unique top categories: {len(top_cat_dist)}\")\n",
    "print(\"Top 5 most common categories: \")\n",
    "print(top_cat_dist.head())\n",
    "\n",
    "print(\"\\nBottom category distribution:\")\n",
    "bottom_cat_dist = train_df['bottom_category_id'].value_counts()\n",
    "print(f\"Number of unique bottom categories: {len(bottom_cat_dist)}\")\n",
    "print(\"Top 5 most common categories: \")\n",
    "print(bottom_cat_dist.head())\n",
    "\n",
    "# Create visualizations directory\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "# VISUALIZATION 1: Top Category Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "top_cat_counts = train_df['top_category_id'].value_counts().head(15).sort_values(ascending=False)\n",
    "sns.barplot(x=top_cat_counts.index, y=top_cat_counts.values)\n",
    "plt.title('Distribution of Top Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Top Category ID')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/top_category_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"\\n=== FEATURE ENGINEERING ===\")\n",
    "print(\"Encoding target variables...\")\n",
    "print(f\"Number of unique top categories: {len(train_df['top_category_id'].unique())}\")\n",
    "print(f\"Number of unique bottom categories: {len(train_df['bottom_category_id'].unique())}\")\n",
    "\n",
    "# Encode target variables\n",
    "top_encoder = LabelEncoder()\n",
    "bottom_encoder = LabelEncoder()\n",
    "\n",
    "train_df['top_category_encoded'] = top_encoder.fit_transform(train_df['top_category_id'])\n",
    "train_df['bottom_category_encoded'] = bottom_encoder.fit_transform(train_df['bottom_category_id'])\n",
    "\n",
    "# Create text features using hashing vectorizer - using fewer features for speed\n",
    "n_features = 2048  # Reduce features for faster processing\n",
    "print(\"Creating text features using hashing vectorizer...\")\n",
    "vectorizer = HashingVectorizer(n_features=n_features, alternate_sign=False)\n",
    "\n",
    "X_train_text = vectorizer.transform(train_df['combined_text'])\n",
    "print(f\"Hashed features shape: {X_train_text.shape}\")\n",
    "\n",
    "# Create target variables\n",
    "y_train_top = train_df['top_category_encoded'].values  # Convert to numpy for faster operations\n",
    "y_train_bottom = train_df['bottom_category_encoded'].values  # Convert to numpy for faster operations\n",
    "\n",
    "# Use a smaller validation set for faster evaluation\n",
    "X_train, X_val, y_train_top, y_val_top, y_train_bottom, y_val_bottom = train_test_split(\n",
    "    X_train_text, y_train_top, y_train_bottom, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"\\n=== MODEL TRAINING - TOP CATEGORY ===\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "\n",
    "# VISUALIZATION 2: Dimensionality Reduction for Text Features\n",
    "print(\"\\nPerforming dimensionality reduction for visualization...\")\n",
    "# Use TruncatedSVD to reduce dimensions for visualization - on a subset of data\n",
    "# Convert to numpy array first to avoid pandas indexing issues\n",
    "y_train_top_np = np.array(y_train_top)\n",
    "\n",
    "# Use only first 2000 samples for visualization for speed\n",
    "sample_size = 2000 \n",
    "X_sample = X_train[:sample_size]\n",
    "y_sample = y_train_top_np[:sample_size]\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_2d = svd.fit_transform(X_sample)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot only the most common categories for clarity\n",
    "unique_categories = np.unique(y_sample)\n",
    "top_categories = unique_categories[:min(5, len(unique_categories))]  # Show only top 5 categories\n",
    "for category in top_categories:\n",
    "    mask = y_sample == category\n",
    "    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], label=f'Cat {category}', alpha=0.6, s=10)\n",
    "\n",
    "plt.title('2D Projection of Text Features (Top 5 Categories)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/text_features_2d.png')\n",
    "plt.close()\n",
    "\n",
    "# Train and evaluate models with optimized parameters for speed\n",
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val, target_name, is_bottom_category=False):\n",
    "    # Configure models for speed (fewer trees, less depth, etc.)\n",
    "    if is_bottom_category:\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=30, max_depth=8, \n",
    "                                                  min_samples_split=100, min_samples_leaf=20,\n",
    "                                                  class_weight='balanced_subsample', \n",
    "                                                  random_state=42, n_jobs=-1),\n",
    "            'SGD Classifier': SGDClassifier(loss='log_loss', penalty='l2', alpha=1e-3, \n",
    "                                          max_iter=3, tol=1e-2, early_stopping=True, \n",
    "                                          n_iter_no_change=2,\n",
    "                                          random_state=42, n_jobs=-1),\n",
    "            'Multinomial Naive Bayes': MultinomialNB(alpha=0.1)\n",
    "        }\n",
    "    else:\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=40, max_depth=10, \n",
    "                                                  min_samples_split=50, min_samples_leaf=10,\n",
    "                                                  random_state=42, n_jobs=-1),\n",
    "            'SGD Classifier': SGDClassifier(loss='log_loss', penalty='l2', alpha=1e-3, \n",
    "                                          max_iter=5, early_stopping=True, \n",
    "                                          random_state=42, n_jobs=-1),\n",
    "            'Multinomial Naive Bayes': MultinomialNB(alpha=0.1)\n",
    "        }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} for {target_name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Set a time limit of 3 minutes (180 seconds) for model training\n",
    "        time_limit = 180  # seconds\n",
    "        \n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # If training takes too long, print warning\n",
    "            if train_time > time_limit:\n",
    "                print(f\"Warning: {name} training took longer than expected ({train_time:.2f}s)\")\n",
    "            \n",
    "            print(f\"Predicting with {name}...\")\n",
    "            y_pred = model.predict(X_val)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'f1_score': f1,\n",
    "                'training_time': train_time\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} F1 Score: {f1:.4f}\")\n",
    "            print(f\"Training time: {train_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            # If an error occurs, still add the model to results with zero F1 score\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'f1_score': 0.0,\n",
    "                'training_time': time.time() - start_time,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train models for top category\n",
    "print(\"\\nTraining models for Top Category Classification...\")\n",
    "top_category_results = train_and_evaluate_models(X_train, X_val, y_train_top, y_val_top, \"Top Category\")\n",
    "\n",
    "# Train models for bottom category with better filtering\n",
    "print(\"\\nTraining models for Bottom Category Classification...\")\n",
    "# Convert to pandas Series for value_counts\n",
    "bottom_class_counts = pd.Series(y_train_bottom).value_counts()\n",
    "\n",
    "# IMPORTANT: Make sure we have at least some samples by using a more reasonable threshold\n",
    "# Find a threshold that gives us at least some classes but not too many\n",
    "for threshold in [50, 25, 10, 5, 2, 1]:\n",
    "    common_bottom_classes = bottom_class_counts[bottom_class_counts >= threshold].index.tolist()\n",
    "    if len(common_bottom_classes) > 0:\n",
    "        break\n",
    "\n",
    "# If we still don't have enough classes, use the most frequent ones\n",
    "if len(common_bottom_classes) == 0:\n",
    "    common_bottom_classes = bottom_class_counts.nlargest(20).index.tolist()\n",
    "elif len(common_bottom_classes) > 200:  # Limit to a reasonable number\n",
    "    common_bottom_classes = bottom_class_counts.nlargest(200).index.tolist()\n",
    "\n",
    "# Filter data to only include common classes\n",
    "mask_train = np.isin(y_train_bottom, common_bottom_classes)\n",
    "mask_val = np.isin(y_val_bottom, common_bottom_classes)\n",
    "\n",
    "X_train_bottom = X_train[mask_train]\n",
    "y_train_bottom_filtered = y_train_bottom[mask_train]\n",
    "X_val_bottom = X_val[mask_val]\n",
    "y_val_bottom_filtered = y_val_bottom[mask_val]\n",
    "\n",
    "print(f\"Using {len(common_bottom_classes)} common bottom categories\")\n",
    "print(f\"Filtered training set shape: {X_train_bottom.shape}\")\n",
    "print(f\"Filtered validation set shape: {X_val_bottom.shape}\")\n",
    "\n",
    "# Make sure we have at least some data\n",
    "if X_train_bottom.shape[0] > 0 and X_val_bottom.shape[0] > 0:\n",
    "    bottom_category_results = train_and_evaluate_models(\n",
    "        X_train_bottom, X_val_bottom, \n",
    "        y_train_bottom_filtered, y_val_bottom_filtered, \n",
    "        \"Bottom Category\", is_bottom_category=True\n",
    "    )\n",
    "else:\n",
    "    print(\"ERROR: Insufficient data for bottom category after filtering\")\n",
    "    # Create dummy results\n",
    "    bottom_category_results = {\n",
    "        'Multinomial Naive Bayes': {\n",
    "            'model': MultinomialNB(),\n",
    "            'f1_score': 0.0,\n",
    "            'training_time': 0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# VISUALIZATION 3: Text Length Distribution\n",
    "train_df['title_length'] = train_df['title'].apply(len)\n",
    "train_df['description_length'] = train_df['description'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train_df['title_length'], kde=True, label='Title Length')\n",
    "plt.title('Distribution of Title Lengths')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/title_length_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# VISUALIZATION 4: Model Performance Comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "model_names = list(top_category_results.keys())\n",
    "f1_scores_top = [results['f1_score'] for results in top_category_results.values()]\n",
    "f1_scores_bottom = [results['f1_score'] for results in bottom_category_results.values()]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, f1_scores_top, width, label='Top Category')\n",
    "plt.bar(x + width/2, f1_scores_bottom, width, label='Bottom Category')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score (Weighted)')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, model_names)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/model_performance_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Prepare test data\n",
    "X_test_text = vectorizer.transform(test_df['combined_text'])\n",
    "\n",
    "# Get the best model for top category prediction\n",
    "best_model_top = max(top_category_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "print(f\"\\nBest model for top category: {best_model_top[0]} (F1: {best_model_top[1]['f1_score']:.4f})\")\n",
    "\n",
    "# Get the best model for bottom category prediction\n",
    "best_model_bottom = max(bottom_category_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "print(f\"Best model for bottom category: {best_model_bottom[0]} (F1: {best_model_bottom[1]['f1_score']:.4f})\")\n",
    "\n",
    "# Make predictions on test data\n",
    "print(\"\\nMaking predictions on test data...\")\n",
    "\n",
    "# For top category, use the best model or fallback to MultinomialNB if issues occur\n",
    "try:\n",
    "    # Use the best model for top category prediction\n",
    "    best_model_top = max(top_category_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "    print(f\"\\nBest model for top category: {best_model_top[0]} (F1: {best_model_top[1]['f1_score']:.4f})\")\n",
    "    y_pred_top = best_model_top[1]['model'].predict(X_test_text)\n",
    "    test_df['predicted_top_category'] = top_encoder.inverse_transform(y_pred_top)\n",
    "except Exception as e:\n",
    "    print(f\"Error predicting top categories: {e}\")\n",
    "    # Fallback - use Multinomial Naive Bayes which is fast and reliable\n",
    "    print(\"Using MultinomialNB fallback for top category\")\n",
    "    mnb = MultinomialNB().fit(X_train, y_train_top)\n",
    "    y_pred_top = mnb.predict(X_test_text)\n",
    "    test_df['predicted_top_category'] = top_encoder.inverse_transform(y_pred_top)\n",
    "\n",
    "# For bottom category, be even more cautious with prediction\n",
    "try:\n",
    "    # First check if we have valid models\n",
    "    best_model_bottom = max(bottom_category_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "    print(f\"Best model for bottom category: {best_model_bottom[0]} (F1: {best_model_bottom[1]['f1_score']:.4f})\")\n",
    "    \n",
    "    # Only proceed if the model actually trained (F1 > 0)\n",
    "    if best_model_bottom[1]['f1_score'] > 0:\n",
    "        y_pred_bottom = best_model_bottom[1]['model'].predict(X_test_text)\n",
    "        \n",
    "        # For bottom categories, need to be more careful with mapping\n",
    "        # Get the most common bottom category to use as a fallback\n",
    "        most_common_bottom = bottom_cat_dist.index[0]\n",
    "        test_df['predicted_bottom_category'] = most_common_bottom  # Default value\n",
    "        \n",
    "        # Map predicted bottom categories\n",
    "        try:\n",
    "            mapped_categories = bottom_encoder.inverse_transform(y_pred_bottom)\n",
    "            test_df['predicted_bottom_category'] = mapped_categories\n",
    "        except Exception as e:\n",
    "            print(f\"Error mapping bottom categories: {e}\")\n",
    "    else:\n",
    "        # Model didn't train properly - use fallback\n",
    "        raise ValueError(\"Bottom category model didn't train properly\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in bottom category prediction: {e}\")\n",
    "    print(\"Using fallback for bottom category prediction...\")\n",
    "    most_common_bottom = bottom_cat_dist.index[0]\n",
    "    test_df['predicted_bottom_category'] = most_common_bottom\n",
    "\n",
    "# Save predictions\n",
    "predictions = test_df[['product_id', 'predicted_top_category', 'predicted_bottom_category']]\n",
    "predictions.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to predictions.csv\")\n",
    "print(\"Visualizations saved to the 'visualizations' directory\")\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "print(\"Top Category Classification:\")\n",
    "for name, results in top_category_results.items():\n",
    "    print(f\"  {name}: F1 Score = {results['f1_score']:.4f}, Training Time = {results['training_time']:.2f}s\")\n",
    "\n",
    "print(\"\\nBottom Category Classification (filtered classes):\")\n",
    "for name, results in bottom_category_results.items():\n",
    "    print(f\"  {name}: F1 Score = {results['f1_score']:.4f}, Training Time = {results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb2945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
